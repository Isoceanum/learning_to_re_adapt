experiment_name: meta_trained_residual_adapter
algo: mlra
env: GymAnt-v0
device: auto # cpu or cuda
exclude_current_positions_from_observation: true

train:
  seed: 42 # Global RNG seed for reproducibility
  max_episode_length: 1000 # Max steps per episode rollout during data collection
  iterations: 10
  steps_per_iteration: 4_000
  valid_split_ratio: 0.1

  pretrained_dynamics_model:
    model_path:  "outputs/2026-01-09-ex3/ant_base_dynamics_steps_10k/model.pt"
    config_path: "outputs/2026-01-09-ex3/ant_base_dynamics_steps_10k/config.yaml"

  residual_adapter:
     hidden_sizes: [128, 128, 128]
     enabled: true

  support_length: 32
  query_length: 32
  meta_batch_size: 50
  inner_steps: 1
  inner_learning_rate: 0.001
  outer_learning_rate: 0.0001
  meta_updates_per_iter: 10

  planner:
    type: "cem" # Planner algorithm; rs, cem 
    horizon: 15 # length of each candidate action sequence evaluated by the planner.
    n_candidates: 250 # Number of candidate action sequences sampled per planning step
    discount: 0.99 # Reward discount factor inside planning rollouts
    num_cem_iters: 4      # CEM iterations per MPC step. Total model rollouts per MPC step â‰ˆ n_candidates * num_cem_iters.
    percent_elites: 0.15    # Fraction of top-return sequences used to refit mean/std each CEM iter (e.g., 0.1 => 10% elites).
    alpha: 0.20            # Smoothing for mean/std updates (with your code: 0.1 = move 10% toward elites, keep 90% previous).

  perturbation:
    type: action_inversion
    probability: 1
    candidate_action_indices: [0]

eval:
  episodes: 1 # Number of episodes to run per seed
  seeds: [0,1,2,3] # Evaluation seeds
  k_list: [1, 2, 5, 10, 15]

  perturbation:
    type: action_inversion
    probability: 1
    candidate_action_indices: [0]