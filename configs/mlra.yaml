experiment_name: meta_trained_residual_adapter
algo: mlra
env: GymAnt-v0
device: auto # cpu or cuda
exclude_current_positions_from_observation: true

train:
  seed: 42 # Global RNG seed for reproducibility
  max_episode_length: 1000 # Max steps per episode rollout during data collection
  iterations: 20
  steps_per_iteration: 10_000
  valid_split_ratio: 0.1
  eval_interval_steps: 10_000

  pretrained_dynamics_model:
    model_path:  "/global/D1/homes/ismailou/l2ra/learning_to_re_adapt/outputs/2026-01-09/ant_base_dynamics_steps_10k/model.pt"
    config_path: "/global/D1/homes/ismailou/l2ra/learning_to_re_adapt/outputs/2026-01-09/ant_base_dynamics_steps_10k/config.yaml"

  residual_adapter:
     hidden_sizes: [128, 128, 128]
     enabled: true

  support_length: 32
  query_length: 32
  meta_batch_size: 128
  inner_steps: 1
  inner_learning_rate: 0.001
  outer_learning_rate: 0.001
  meta_updates_per_iter: 10

  planner:
    type: "cem" # Planner algorithm; rs, cem 
    horizon: 15 # length of each candidate action sequence evaluated by the planner.
    n_candidates: 500 # Number of candidate action sequences sampled per planning step
    discount: 0.97 # Reward discount factor inside planning rollouts
    num_cem_iters: 4      # CEM iterations per MPC step. Total model rollouts per MPC step â‰ˆ n_candidates * num_cem_iters.
    percent_elites: 0.10    # Fraction of top-return sequences used to refit mean/std each CEM iter (e.g., 0.1 => 10% elites).
    alpha: 0.20            # Smoothing for mean/std updates (with your code: 0.1 = move 10% toward elites, keep 90% previous).

  perturbation:
    type: action_inversion
    probability: 1
    candidate_action_indices: [0,1,2,3,4,5,6,7]

eval:
  episodes: 10 # Number of episodes to run per seed
  seeds: [0,1,2,3,4,5,6,7,8,9] # Evaluation seeds
  k_list: [1, 2, 5, 10, 15]

  perturbation:
    type: action_inversion
    probability: 1
    candidate_action_indices: [0,1,2,3,4,5,6,7]