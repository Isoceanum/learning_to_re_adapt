experiment_name: halfcheetah_mbmpc_1
algo: mb_mpc_nagabandi
env: HalfCheetahCustom-v0

train:
  # Shorter run (aim ~1â€“2 hours on a single GPU),
  # while still showing improving mean reward.
  total_iterations: 8
  init_random_steps: 1500
  rollout_steps: 6000
  epochs: 20

  # Dynamics + planner hyperparameters
  hidden_sizes: [512, 512]
  lr: 0.001
  batch_size: 500
  val_ratio: 0.2
  device: cuda

  # CEM settings
  horizon: 10
  n_candidates: 256
  num_cem_iters: 5
  percent_elites: 0.1
  alpha: 0.1  # mean = alpha*old + (1-alpha)*new
  # Planner options
  warm_start: false        # reuse previous mean sequence across timesteps
  clip_rollouts: false     # clip actions during rollouts (original used unclipped)

  n_envs: 8

eval:
  episodes: 10
  seeds: [0, 1, 2, 3, 4, 5]
  deterministic: true
  save_csv: true
