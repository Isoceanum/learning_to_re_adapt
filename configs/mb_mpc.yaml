experiment_name: halfcheetah_mbmpc__baseline
algo: mb_mpc
env: HalfCheetahCustom-v0


train:
  # MB-MPC training loop settings
  total_iterations: 30        # number of dataset aggregation iterations
  init_random_steps: 5000     # steps with random actions (iteration 1)
  rollout_steps: 1000         # steps using planner in subsequent iterations
  epochs: 50                  # dynamics model epochs per iteration

  # Dynamics + planner hyperparameters
  hidden_sizes: [256, 256]
  lr: 0.001
  batch_size: 256
  val_ratio: 0.1
  horizon: 30
  num_candidates: 5000
  device: cuda                 # or "cuda" if available
  ensemble_size: 5            # set to 1 for baseline, 5 for uncertainty
  # ctrl_cost_weight: 0.001   # optional; env default is used if present

  # Planner knobs
  num_elites: 500             # elites per CEM iteration
  max_iters: 5                # CEM iterations
  alpha: 0.2                  # smoothing for mean/std updates
  particles: 1                # TSâˆž particles per candidate
  aggregate: mean             # mean | risk_averse
  risk_coef: 0.0              # only used if aggregate=risk_averse

eval:
  episodes: 4
  seeds: [0, 1, 2, 3]
  deterministic: true
  save_csv: true
