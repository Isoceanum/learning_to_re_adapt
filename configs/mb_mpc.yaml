experiment_name: halfcheetah_mbmpc__baseline_nagabandi
algo: mb_mpc
env: HalfCheetahCustom-v0

train:
  # MB-MPC training loop settings
  total_iterations: 50          # n_itr in the paper
  init_random_steps: 5000       # initial random samples (approximate)
  rollout_steps: 32000          # #TS/itr (samples per iteration)
  epochs: 50                    # dynamics model epochs per iteration

  # Dynamics + planner hyperparameters
  hidden_sizes: [512, 512]      # bigger nets (Nagabandi used 512)
  lr: 0.001
  batch_size: 500               # paper baseline batch size
  val_ratio: 0.1
  horizon: 10                   # H_train in Table 3
  num_candidates: 2000          # reported baseline
  device: cuda
  ensemble_size: 5              # typical PETS setting; consistent with paper’s intent
  # ctrl_cost_weight: 0.001

  # Planner knobs
  num_elites: 200               # ~10% of candidates
  max_iters: 5                  # CEM iterations
  alpha: 0.2                    # smoothing for mean/std updates
  particles: 1                  # TS1/∞ doesn’t matter here, paper used 1
  aggregate: mean
  risk_coef: 0.0

eval:
  episodes: 10                  # more robust eval
  seeds: [0, 1, 2, 3, 4]        # 5 seeds like paper’s GrBAL/ReBAL eval
  deterministic: true
  save_csv: true
